{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c1ad72",
   "metadata": {},
   "source": [
    "# RAG Pipeline with Incremental Indexing\n",
    "This notebook demonstrates how to combine indexing and RAG pipelines to process URLs one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829a8456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: haystack-ai in c:\\users\\xdn13\\appdata\\roaming\\python\\python311\\site-packages (2.18.0)\n",
      "Requirement already satisfied: docstring-parser in d:\\python3.11\\lib\\site-packages (from haystack-ai) (0.17.0)\n",
      "Requirement already satisfied: filetype in d:\\python3.11\\lib\\site-packages (from haystack-ai) (1.2.0)\n",
      "Requirement already satisfied: haystack-experimental in d:\\python3.11\\lib\\site-packages (from haystack-ai) (0.13.0)\n",
      "Requirement already satisfied: jinja2 in d:\\python3.11\\lib\\site-packages (from haystack-ai) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in d:\\python3.11\\lib\\site-packages (from haystack-ai) (4.25.1)\n",
      "Requirement already satisfied: lazy-imports in d:\\python3.11\\lib\\site-packages (from haystack-ai) (1.0.1)\n",
      "Requirement already satisfied: more-itertools in d:\\python3.11\\lib\\site-packages (from haystack-ai) (10.8.0)\n",
      "Requirement already satisfied: networkx in d:\\python3.11\\lib\\site-packages (from haystack-ai) (3.5)\n",
      "Requirement already satisfied: numpy in d:\\python3.11\\lib\\site-packages (from haystack-ai) (2.3.3)\n",
      "Requirement already satisfied: openai>=1.56.1 in d:\\python3.11\\lib\\site-packages (from haystack-ai) (1.107.2)\n",
      "Requirement already satisfied: posthog!=3.12.0 in d:\\python3.11\\lib\\site-packages (from haystack-ai) (6.7.4)\n",
      "Requirement already satisfied: pydantic in d:\\python3.11\\lib\\site-packages (from haystack-ai) (2.11.7)\n",
      "Requirement already satisfied: python-dateutil in d:\\python3.11\\lib\\site-packages (from haystack-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml in d:\\python3.11\\lib\\site-packages (from haystack-ai) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\python3.11\\lib\\site-packages (from haystack-ai) (2.32.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0 in d:\\python3.11\\lib\\site-packages (from haystack-ai) (9.1.2)\n",
      "Requirement already satisfied: tqdm in d:\\python3.11\\lib\\site-packages (from haystack-ai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\python3.11\\lib\\site-packages (from haystack-ai) (4.15.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\python3.11\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\python3.11\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\python3.11\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\python3.11\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in d:\\python3.11\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\python3.11\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.56.1->haystack-ai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\python3.11\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.56.1->haystack-ai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\python3.11\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.56.1->haystack-ai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\python3.11\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.56.1->haystack-ai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\python3.11\\lib\\site-packages (from pydantic->haystack-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\python3.11\\lib\\site-packages (from pydantic->haystack-ai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\python3.11\\lib\\site-packages (from pydantic->haystack-ai) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python3.11\\lib\\site-packages (from posthog!=3.12.0->haystack-ai) (1.17.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in d:\\python3.11\\lib\\site-packages (from posthog!=3.12.0->haystack-ai) (2.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\python3.11\\lib\\site-packages (from requests->haystack-ai) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python3.11\\lib\\site-packages (from requests->haystack-ai) (2.4.0)\n",
      "Requirement already satisfied: colorama in d:\\python3.11\\lib\\site-packages (from tqdm->haystack-ai) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python3.11\\lib\\site-packages (from jinja2->haystack-ai) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\python3.11\\lib\\site-packages (from jsonschema->haystack-ai) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\python3.11\\lib\\site-packages (from jsonschema->haystack-ai) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\python3.11\\lib\\site-packages (from jsonschema->haystack-ai) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\python3.11\\lib\\site-packages (from jsonschema->haystack-ai) (0.25.1)\n",
      "Requirement already satisfied: datasets>=3.6.0 in d:\\python3.11\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (2.3.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in d:\\python3.11\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (0.33.0)\n",
      "Requirement already satisfied: packaging in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python3.11\\lib\\site-packages (from datasets>=3.6.0) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\python3.11\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (3.12.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\python3.11\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in d:\\python3.11\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.6.0) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python3.11\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets>=3.6.0) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\python3.11\\lib\\site-packages (from requests>=2.32.2->datasets>=3.6.0) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python3.11\\lib\\site-packages (from requests>=2.32.2->datasets>=3.6.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python3.11\\lib\\site-packages (from requests>=2.32.2->datasets>=3.6.0) (2025.4.26)\n",
      "Requirement already satisfied: colorama in d:\\python3.11\\lib\\site-packages (from tqdm>=4.66.3->datasets>=3.6.0) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python3.11\\lib\\site-packages (from pandas->datasets>=3.6.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python3.11\\lib\\site-packages (from pandas->datasets>=3.6.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\python3.11\\lib\\site-packages (from pandas->datasets>=3.6.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python3.11\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.6.0) (1.17.0)\n",
      "Requirement already satisfied: sentence-transformers>=4.1.0 in d:\\python3.11\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (4.52.4)\n",
      "Requirement already satisfied: tqdm in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (1.7.0)\n",
      "Requirement already satisfied: scipy in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (0.33.0)\n",
      "Requirement already satisfied: Pillow in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\python3.11\\lib\\site-packages (from sentence-transformers>=4.1.0) (4.15.0)\n",
      "Requirement already satisfied: filelock in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\python3.11\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\python3.11\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=4.1.0) (2025.3.0)\n",
      "Requirement already satisfied: networkx in d:\\python3.11\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=4.1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\python3.11\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=4.1.0) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\python3.11\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=4.1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\python3.11\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=4.1.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\python3.11\\lib\\site-packages (from tqdm->sentence-transformers>=4.1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python3.11\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=4.1.0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\python3.11\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python3.11\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python3.11\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python3.11\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=4.1.0) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\python3.11\\lib\\site-packages (from scikit-learn->sentence-transformers>=4.1.0) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\python3.11\\lib\\site-packages (from scikit-learn->sentence-transformers>=4.1.0) (3.6.0)\n",
      "Requirement already satisfied: accelerate in d:\\python3.11\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in d:\\python3.11\\lib\\site-packages (from accelerate) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python3.11\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in d:\\python3.11\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in d:\\python3.11\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\python3.11\\lib\\site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in d:\\python3.11\\lib\\site-packages (from accelerate) (0.33.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\python3.11\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in d:\\python3.11\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\python3.11\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in d:\\python3.11\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\python3.11\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python3.11\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: networkx in d:\\python3.11\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\python3.11\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\python3.11\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\python3.11\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\python3.11\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python3.11\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\python3.11\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python3.11\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\python3.11\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python3.11\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "!pip install haystack-ai\n",
    "!pip install \"datasets>=3.6.0\"\n",
    "!pip install \"sentence-transformers>=4.1.0\"\n",
    "!pip install accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4c8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.utils.auth import Secret\n",
    "from haystack.components.converters import HTMLToDocument\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from haystack_integrations.components.embedders.cohere import CohereDocumentEmbedder, CohereTextEmbedder\n",
    "from haystack.components.evaluators import ContextRelevanceEvaluator,FaithfulnessEvaluator\n",
    "from haystack.components.evaluators import SASEvaluator\n",
    "from haystack.components.builders import PromptBuilder, AnswerBuilder\n",
    "from haystack_integrations.components.rankers.cohere import CohereRanker\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# ä».envæ–‡ä»¶è¯»å–API key\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = cohere_api_key if cohere_api_key else \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key if openai_api_key else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a98202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# è¯»å–æœ¬åœ° index_table.json æ–‡ä»¶\n",
    "with open(\"../files/index_table.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d0e93",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d5e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a privacy policy expert. You are provided with {{app_url}}, which contains the privacy policy document for an app.\n",
    "Your task is to:\n",
    " - answer the question based on the privacy policy document,\n",
    " - provide references for your answers based on the section in the privacy policy document from which your answer is generated,\n",
    " - produce your results strictly in the JSON format below (no extra text beyond JSON),\n",
    " - ensure that the 'url' in the 'meta' section is exactly {{app_url}}.\n",
    "\n",
    "JSON format:\n",
    "{\n",
    "   \"meta\": {\n",
    "       \"id\": {{ app_id }},\n",
    "       \"url\": {{ app_url }},\n",
    "       \"title\": {{ app_name | tojson }}\n",
    "   },\n",
    "   \"reply\": {\n",
    "       \"qid\": \"{{ qid }}\",\n",
    "       \"question\": \"{{ question | escape }}\",\n",
    "       \"answer\": {\n",
    "           \"full_answer\": \"{{ full_answer | escape }}\",\n",
    "           \"simple_answer\": \"{{ simple_answer | escape }}\",\n",
    "           \"extended_simple_answer\": {\n",
    "               \"comment\": \"{{ extended_comment | escape }}\",\n",
    "               \"content\": \"{{ extended_content | escape }}\"\n",
    "           }\n",
    "       },\n",
    "       \"analysis\": \"{{ analysis | escape }}\",\n",
    "       \"reference\": \"{{ reference | escape }}\"\n",
    "   }\n",
    "}\n",
    "\n",
    "\n",
    "Instructions:\n",
    "1. Approach each question systematically:\n",
    "   a. Understand the question: Break down the question into specific components or sub-questions if needed.\n",
    "\n",
    "   b. Identify relevant context from the privacy policy.\n",
    "   c. Analyze the context and link it back to the question.\n",
    "   d. Formulate the answer for each JSON field.\n",
    "   e. Provide references:  (original text + 'URL: {{app_url}}'). If none, report 'N/A. URL: {{app_url}}'. Note that the original text must come from only the relevant context in page {{app_url}}\n",
    "\n",
    "2. Output structure:\n",
    "   - full_answer: must integrate info from both simple_answer and extended_simple_answer. The full answer section must not be empty.\n",
    "   - simple_answer: follow the rules above. This section must not be empty.\n",
    "   - extended_simple_answer: follow the rules above, or empty if not specified\n",
    "   - analysis: describe your reasoning\n",
    "   - reference: original text snippets + URL. The context must come from the app URL {{app_url}}. Attach the {{app_url}} in the end. Ensure JSON compatibility by replacing double quotes with single quotes.\n",
    "\n",
    "\n",
    "3. Special rules for the `simple_answer` and `extended_simple_answer` fields:\n",
    "   - If the question is â€œ1. Does the app declare the collection of data?â€:\n",
    "       * simple_answer: \"Yes\" or \"No\"\n",
    "       * extended_simple_answer: leave empty\n",
    "\n",
    "   - If the question is â€œ2. If the app declares the collection of data, what type of data does it collect?':\n",
    "       * simple_answer: \"NOTUSED\"\n",
    "       * extended_simple_answer: \n",
    "           - comment: \"data collected\"\n",
    "           - content: list of data types collected\n",
    "       * full_answer: MUST be a concise natural language synthesis of the listed data types (DO NOT use \"NOTUSED\" here; never leave it empty).\n",
    "\n",
    "   - If the question is \"3. Does the app declare the purpose of data collection and use?\":\n",
    "       * simple_answer: \"Yes\" or \"No\"\n",
    "       * extended_simple_answer: leave empty\n",
    "\n",
    "   - If the question is \"4. Can the user opt out of data collection or delete data?\":\n",
    "       * simple_answer: \"Yes\" or \"No\"\n",
    "       * extended_simple_answer: leave empty\n",
    "\n",
    "   - If the question is \"5. Does the app share data with third parties?\":\n",
    "       * simple_answer: \"Yes\" or \"No\"\n",
    "       * extended_simple_answer: leave empty\n",
    "\n",
    "   - If the question is \"6. If the app shares data with third parties, what third parties does the app share data with?\": \n",
    "       * simple_answer: \"NOTUSED\"\n",
    "       * extended_simple_answer:\n",
    "           - comment: \"third parties\"\n",
    "           - content: list of third parties\n",
    "       * full_answer: MUST be a concise natural language synthesis of the listed data types (DO NOT use \"NOTUSED\" here; never leave it empty).\n",
    "\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "  {{ doc.content }}\n",
    "  URL: {{ doc.meta['url'] }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ query }}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbf78e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'outputs/'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "436c1538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\xdn13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Initialize document store and pipelines\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# ===================== é™æ€æ— çŠ¶æ€ç»„ä»¶ï¼ˆå¾ªç¯å¤–å»ºä¸€æ¬¡ï¼‰ =====================\n",
    "# ä¸ºé˜²æ­¢éƒ¨åˆ†ç«™ç‚¹æ‹’ç»é»˜è®¤UAï¼Œæ·»åŠ æµè§ˆå™¨UAï¼Œå¹¶åœ¨å¤±è´¥æ—¶ä¸æŠ›å‡ºå¼‚å¸¸\n",
    "fetcher = LinkContentFetcher(raise_on_failure=False)\n",
    "converter = HTMLToDocument()\n",
    "cleaner = DocumentCleaner()\n",
    "\n",
    "splitter = DocumentSplitter(\n",
    "    split_by=\"word\",\n",
    "    split_length=220,\n",
    "    split_overlap=50\n",
    ")\n",
    "\n",
    "embedder = CohereDocumentEmbedder(\n",
    "    model=\"embed-english-v3.0\",\n",
    "    api_base_url=os.getenv(\"CO_API_URL\")\n",
    ")\n",
    "query_embedder = CohereTextEmbedder(\n",
    "    model=\"embed-english-v3.0\",\n",
    "    api_base_url=os.getenv(\"CO_API_URL\")\n",
    ")\n",
    "\n",
    "prompt_builder = PromptBuilder(\n",
    "    template=prompt,\n",
    "    required_variables=[\"query\", \"app_id\", \"app_url\", \"question\", \"documents\"]\n",
    ")\n",
    "generator = OpenAIGenerator(model=\"gpt-3.5-turbo\")\n",
    "answer_builder = AnswerBuilder()\n",
    "\n",
    "\n",
    "reranker = CohereRanker(model=\"rerank-english-v3.0\", top_k=5)\n",
    "\n",
    "\n",
    "# ===================== å ä½çš„å¯å˜ç»„ä»¶ï¼ˆå…ˆç»‘å®šä¸€ä¸ªä¸´æ—¶ storeï¼‰ =====================\n",
    "# å…³é”®ï¼šwriter / retriever çš„å®ä¾‹æ˜¯â€œå¯å˜â€çš„ï¼Œæˆ‘ä»¬æ¯è½®æ›¿æ¢å®ƒä»¬çš„ document_store å³å¯\n",
    "_initial_store = InMemoryDocumentStore()\n",
    "writer = DocumentWriter(document_store=_initial_store, policy=DuplicatePolicy.OVERWRITE)\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=_initial_store, top_k=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d99054a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x000001A86182F450>\n",
       "ğŸš… Components\n",
       "  - query_embedder: CohereTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - reranker: CohereRanker\n",
       "  - prompt: PromptBuilder\n",
       "  - generator: OpenAIGenerator\n",
       "  - answer_builder: AnswerBuilder\n",
       "ğŸ›¤ï¸ Connections\n",
       "  - query_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> reranker.documents (list[Document])\n",
       "  - reranker.documents -> prompt.documents (List[Document])\n",
       "  - reranker.documents -> answer_builder.documents (List[Document])\n",
       "  - prompt.prompt -> generator.prompt (str)\n",
       "  - generator.replies -> answer_builder.replies (list[str])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===================== Pipeline åªæ„å»ºä¸€æ¬¡ï¼ˆæ—  Noneï¼‰ =====================\n",
    "# ç´¢å¼•ç®¡é“ï¼šfetch -> convert -> clean -> split -> embed -> write\n",
    "indexing = Pipeline()\n",
    "indexing.add_component(\"fetcher\", fetcher)\n",
    "indexing.add_component(\"converter\", converter)\n",
    "indexing.add_component(\"cleaner\", cleaner)\n",
    "indexing.add_component(\"splitter\", splitter)\n",
    "indexing.add_component(\"embedder\", embedder)\n",
    "indexing.add_component(\"writer\", writer)  # æ³¨æ„ï¼šæ˜¯çœŸå® writerï¼Œä¸æ˜¯ None\n",
    "\n",
    "indexing.connect(\"fetcher.streams\", \"converter.sources\")\n",
    "indexing.connect(\"converter\", \"cleaner\")\n",
    "indexing.connect(\"cleaner\", \"splitter\")\n",
    "indexing.connect(\"splitter\", \"embedder\")\n",
    "indexing.connect(\"embedder\", \"writer\")\n",
    "\n",
    "# RAG ç®¡é“ï¼šquery_embedder -> retriever -> prompt -> generator -> answer_builder\n",
    "rag = Pipeline()\n",
    "rag.add_component(\"query_embedder\", query_embedder)\n",
    "rag.add_component(\"retriever\", retriever)  # æ³¨æ„ï¼šæ˜¯çœŸå® retrieverï¼Œä¸æ˜¯ None\n",
    "rag.add_component(\"reranker\", reranker)    # âœ… æ–°å¢é‡æ’å™¨\n",
    "rag.add_component(\"prompt\", prompt_builder)\n",
    "rag.add_component(\"generator\", generator)\n",
    "rag.add_component(\"answer_builder\", answer_builder)\n",
    "\n",
    "rag.connect(\"query_embedder.embedding\", \"retriever.query_embedding\")\n",
    "# âœ… å…ˆå¬å› â†’ å†é‡æ’ â†’ å†é€ç»™ prompt / answer_builder\n",
    "rag.connect(\"retriever.documents\", \"reranker.documents\")\n",
    "rag.connect(\"reranker.documents\", \"prompt.documents\")\n",
    "rag.connect(\"reranker.documents\", \"answer_builder.documents\")\n",
    "rag.connect(\"prompt\", \"generator\")\n",
    "rag.connect(\"generator.replies\", \"answer_builder.replies\")\n",
    "# ï¼ˆä¸è¦è¿ query_embedder.text -> answer_builder.queryï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68c621",
   "metadata": {},
   "source": [
    "## Use LLM generate replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Retrieval short query mapping (Step B added) =====================\n",
    "# Note: These are \"short phrases for vector retrieval\", removing template words and keeping only semantic core, currently not used\n",
    "SHORT_QUERY_PROMPTS = {\n",
    "    # ... (mapping skipped for brevity, currently not used)\n",
    "}\n",
    "\n",
    "# Define output folder\n",
    "output_dir = \"outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Allow user to customize selection quantity  \n",
    "num_to_process = 1  # Modify this to set how many URLs to process\n",
    "if num_to_process:\n",
    "    data = data[:num_to_process]\n",
    "# Document excerpt length setting\n",
    "excerpt_max_len = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback for answer_json construction, immediately add this \"secondary repair\"\n",
    "def repair_answer_json(answer_json):\n",
    "    fa = answer_json.get(\"full_answer\", \"\")\n",
    "    # Only try if full_answer is string and looks like JSON\n",
    "    if isinstance(fa, str) and fa.strip().startswith(\"{\"):\n",
    "        s = fa.strip()\n",
    "        # 1) Remove common \"trailing commas\"\n",
    "        s = re.sub(r\",\\s*}\", \"}\", s)\n",
    "        # 2) Remove fences ```json ... ```\n",
    "        s = re.sub(r\"```json\\s*\", \"\", s)\n",
    "        s = re.sub(r\"```\\s*$\", \"\", s).strip()\n",
    "        try:\n",
    "            cand = json.loads(s)\n",
    "            # If cand itself is our expected structure (contains reply/answer), use it directly\n",
    "            if \"reply\" in cand and \"answer\" in cand[\"reply\"]:\n",
    "                # Preserve existing qid to avoid losing question number\n",
    "                if \"qid\" in answer_json and \"qid\" not in cand[\"reply\"]:\n",
    "                    cand[\"reply\"][\"qid\"] = answer_json[\"qid\"]\n",
    "                return cand\n",
    "        except:\n",
    "            pass\n",
    "    return answer_json\n",
    "\n",
    "# Generate candidate fetch URLs (http/https, trailing slash, common privacy paths)\n",
    "def expand_url_variants(url_raw: str) -> List[str]:\n",
    "    variants = []\n",
    "    u = url_raw.strip()\n",
    "    if not u:\n",
    "        return variants\n",
    "    variants.append(u)  # Original\n",
    "    # Trailing slash variant\n",
    "    if u.endswith(\"/\"):\n",
    "        variants.append(u.rstrip(\"/\"))\n",
    "    else:\n",
    "        variants.append(u + \"/\")\n",
    "    # http variant\n",
    "    if u.startswith(\"https://\"):\n",
    "        variants.append(u.replace(\"https://\", \"http://\", 1))\n",
    "    elif u.startswith(\"http://\"):\n",
    "        variants.append(u.replace(\"http://\", \"https://\", 1))\n",
    "    # Common privacy paths (if given site homepage or wrong path)\n",
    "    from urllib.parse import urlparse\n",
    "    parsed = urlparse(u)\n",
    "    base = f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "    for path_suffix in [\"/privacy\", \"/privacy-policy\", \"/privacypolicy\"]:\n",
    "        variants.append(base + path_suffix)\n",
    "    return list(dict.fromkeys(variants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecdecb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Document 1/1: None (1361356590)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.53it/s]\n",
      "|Processing Questions for None:  17%|â–ˆâ–‹        | 1/6 [00:04<00:21,  4.27s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        1. Does the app declare the collection of data?\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|Processing Questions for None:  33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [00:07<00:14,  3.62s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        2. If the app declares the collection of data, what type of data does it collect?\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|Processing Questions for None:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:10<00:10,  3.34s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 3 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        3. Does the app declare the purpose of data collection and use?\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|Processing Questions for None:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:14<00:06,  3.44s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 4 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        4. Can the user opt out of data collection or delete data?\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|Processing Questions for None:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5/6 [00:18<00:03,  3.78s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 5 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        5. Does the app share data with third parties?\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|Processing Questions for None: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:21<00:00,  3.58s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 6 answered using 5 document (forced single)\n",
      "Query: \n",
      "        You are analyzing the 'None' with URL: http://www.balanceapp.com/balance-privacy.html.\n",
      "        Answer the following questions based on the privacy policy document:\n",
      "        6. If the app shares data with third parties, what third parties does the app share data with?\n",
      "        \n",
      "\n",
      "Saved answers for App ID 1361356590 to outputs/1361356590.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================== ä¸»å¾ªç¯ï¼šæ¯ä¸ª App ç‹¬ç«‹ storeï¼Œä½† pipeline ä¸é‡å»º =====================\n",
    "for idx, item in enumerate(data[:num_to_process]):\n",
    "    app_id = item.get('id')\n",
    "    app_name = item.get('title')\n",
    "    app_url = item.get('url')\n",
    "    if not app_url:\n",
    "        print(f\"No URL found for App ID: {app_id}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing Document {idx + 1}/{num_to_process}: {app_name} ({app_id})\")\n",
    "\n",
    "    # â€”â€” æ ¸å¿ƒï¼šä¸ºå½“å‰ App åˆ›å»ºç‹¬ç«‹çš„ storeï¼Œå¹¶â€œå°±åœ°æ›¿æ¢â€ writer/retriever æ‰€å¼•ç”¨çš„ store â€”â€” #\n",
    "    current_store = InMemoryDocumentStore()\n",
    "    writer.document_store = current_store          # ç´¢å¼•å†™å…¥åˆ°å½“å‰ store\n",
    "    retriever.document_store = current_store       # æ£€ç´¢ä»å½“å‰ store è¯»å–\n",
    "\n",
    "\n",
    "    # ç´¢å¼•å½“å‰ app çš„é¡µé¢ï¼ˆå«å¤šURLå›é€€ï¼Œé¿å… 400/403/404 ç›´æ¥å¤±è´¥ï¼‰\n",
    "    success = False\n",
    "    for try_url in _generate_candidate_urls(app_url):\n",
    "        try:\n",
    "            _ = indexing.run({\"fetcher\": {\"urls\": [try_url]}})\n",
    "            # è‹¥å†™å…¥æˆåŠŸåº”æœ‰æ–‡æ¡£\n",
    "            if getattr(current_store, \"count_documents\", None):\n",
    "                if current_store.count_documents() > 0:\n",
    "                    success = True\n",
    "                    if try_url != app_url:\n",
    "                        print(f\"[indexing] Fallback URL used: {try_url}\")\n",
    "                    break\n",
    "            else:\n",
    "                # ä¸æ”¯æŒè®¡æ•°åˆ™ä»¥ä¸æŠ¥é”™è§†ä¸ºæˆåŠŸ\n",
    "                success = True\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"[indexing] Failed on {try_url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not success:\n",
    "        print(f\"âš ï¸ Unable to fetch any content for App ID {app_id}. Skipping this app.\")\n",
    "        continue\n",
    "\n",
    "    answers_list = []\n",
    "\n",
    "    for j in tqdm(range(len(questions)), desc=f\"|Processing Questions for {app_name}\", unit=\"question\"):\n",
    "        query = f\"\"\"\n",
    "        You are analyzing the '{app_name}' with URL: {app_url}.\n",
    "        Answer the following questions based on the privacy policy document:\n",
    "        {questions[j]}\n",
    "        \"\"\"\n",
    "        # Step B: ä¸ºæ£€ç´¢æ„é€ â€œçŸ­æŸ¥è¯¢â€ â€”â€” æ›´èšç„¦ã€æ›´ç¨³å¥\n",
    "        retriever_query = query_map.get(j, questions[j])\n",
    "\n",
    "        result = rag.run({\n",
    "            # âœ… ç”¨çŸ­æŸ¥è¯¢è¿›è¡Œå‘é‡å¬å›\n",
    "            \"query_embedder\": {\"text\": questions[j]},\n",
    "            # âœ… æ”¾å¤§å¬å›æ± ï¼›é‡æ’å™¨ä¼šå‹åˆ° top_k=5ï¼ˆè§ä¸Šé¢çš„ reranker.top_kï¼‰\n",
    "            \"retriever\": {\"top_k\": 15},\n",
    "            \"reranker\": {\"query\": questions[j]},  # âœ… ä¸º CohereRanker æä¾›å¿…éœ€çš„ query\n",
    "            \"prompt\": {\"qid\": f\"q{j+1}\", \"query\": query, \"app_id\": app_id, \"app_url\": app_url, \"question\": questions[j]},\n",
    "            \"answer_builder\": {\"query\": query}\n",
    "        })\n",
    "\n",
    "        generated_answers = result['answer_builder']['answers']\n",
    "\n",
    "        source_docs_export = []\n",
    "        retrieved_context_list = []\n",
    "\n",
    "        if generated_answers:\n",
    "            structured_answer = generated_answers[0]\n",
    "            answer = structured_answer.data\n",
    "            source_documents = structured_answer.documents\n",
    "\n",
    "            print(f\"Question {j+1} answered using {len(source_documents)} document (forced single)\")\n",
    "            print(f\"Query: {structured_answer.query}\")\n",
    "\n",
    "            for d in source_documents:\n",
    "                excerpt = (d.content or \"\")[:500]\n",
    "                if len(d.content or \"\") > 500: excerpt += \"...\"\n",
    "                source_docs_export.append({\n",
    "                    \"id\": getattr(d, 'id', None),\n",
    "                    \"score\": getattr(d, 'score', None),\n",
    "                    \"excerpt\": excerpt,\n",
    "                    \"url\": (getattr(d, 'meta', {}) or {}).get('url')\n",
    "                })\n",
    "                retrieved_context_list.append(excerpt)\n",
    "        else:\n",
    "            # å›é€€åˆ°ç›´æ¥ä½¿ç”¨ç”Ÿæˆå™¨è¾“å‡º\n",
    "            answer = result['generator']['replies'][0]\n",
    "            retrieved_context_list.append(\"No context available\")\n",
    "\n",
    "        # è§£æä¸å­˜å‚¨ç­”æ¡ˆ\n",
    "        try:\n",
    "            answer_json = json.loads(answer)\n",
    "        except json.JSONDecodeError:\n",
    "            # å…œåº•ï¼šä¿ç•™é¢˜å·ä¸åŸæ–‡ï¼Œé¿å…è¯„ä¼°é˜¶æ®µâ€œæœªæ‰¾åˆ°é—®é¢˜ qX çš„RAGè¾“å‡ºâ€\n",
    "            answer_json = {\n",
    "                \"meta\": {\"id\": app_id, \"url\": app_url},\n",
    "                \"reply\": {\n",
    "                    \"qid\": f\"q{j+1}\",\n",
    "                    \"question\": questions[j],\n",
    "                    \"answer\": {\n",
    "                        \"full_answer\": str(answer),\n",
    "                        \"simple_answer\": \"\",\n",
    "                        \"extended_simple_answer\": {\"comment\": \"\", \"content\": \"\"}\n",
    "                    },\n",
    "                    \"_parsing_note\": \"model_output_not_valid_json_fallback_used\"\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # åœ¨ä½ è®¾ç½®å®Œå…œåº• answer_json ä¹‹åè°ƒç”¨ï¼š\n",
    "        answer_json = _try_salvage_nested_json(answer_json)\n",
    "\n",
    "        # å®‰å…¨åˆå¹¶æ¥æºç‰‡æ®µï¼ˆå¦‚æœ‰ï¼‰\n",
    "        try:\n",
    "            if source_docs_export:\n",
    "                answer_json.setdefault(\"source_documents\", source_docs_export)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        answers_list.append(answer_json)\n",
    "\n",
    "\n",
    "\n",
    "    # ä¿å­˜ JSON\n",
    "    output_file_path = os.path.join(output_folder, f\"{app_id}.json\")\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(answers_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"\\nSaved answers for App ID {app_id} to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f7955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ åˆ›å»ºåŸºäº5ä¸ªéšç§æ”¿ç­–é—®é¢˜çš„ä¸“é—¨RAGè¯„ä¼°Pipeline...\n",
      "ğŸ¯ å¼€å§‹æ‰§è¡Œéšç§æ”¿ç­–RAGè¯„ä¼°...\n",
      "================================================================================\n",
      "ğŸš€ å¼€å§‹éšç§æ”¿ç­–RAGç³»ç»Ÿç»¼åˆè¯„ä¼°\n",
      "================================================================================\n",
      "âœ… åŠ è½½äº† 10 ä¸ªåº”ç”¨çš„groundtruthæ ‡æ³¨\n",
      "ğŸ“Š æ‰¾åˆ° 9 ä¸ªå¯è¯„ä¼°çš„åº”ç”¨\n",
      "ğŸ¯ å°†è¯„ä¼°å‰ 9 ä¸ªåº”ç”¨: [1361356590, 1435692352, 1458846512, 1493155192, 1498229813, 1588978095, 1665348316, 6447095050, 6474216442]\n",
      "\n",
      "ğŸ“‹ è¯„ä¼°åº”ç”¨ ID: 1361356590\n",
      "âœ… æ‰¾åˆ° 6 ä¸ªé—®é¢˜çš„è¾“å‡º\n",
      "ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:12<00:00,  2.11s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:10<00:00,  1.76s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Haystackè¯„ä¼°å®Œæˆ\n",
      "\n",
      "ğŸ“‹ è¯„ä¼°åº”ç”¨ ID: 1435692352\n",
      "âœ… æ‰¾åˆ° 6 ä¸ªé—®é¢˜çš„è¾“å‡º\n",
      "ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.33s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.45s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Haystackè¯„ä¼°å®Œæˆ\n",
      "\n",
      "ğŸ“‹ è¯„ä¼°åº”ç”¨ ID: 1458846512\n",
      "âœ… æ‰¾åˆ° 6 ä¸ªé—®é¢˜çš„è¾“å‡º\n",
      "ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:12<00:00,  2.09s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:11<00:00,  1.98s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Haystackè¯„ä¼°å®Œæˆ\n",
      "\n",
      "ğŸ“‹ è¯„ä¼°åº”ç”¨ ID: 1493155192\n",
      "âœ… æ‰¾åˆ° 6 ä¸ªé—®é¢˜çš„è¾“å‡º\n",
      "ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:13<00:00,  2.18s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:09<00:00,  1.60s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Haystackè¯„ä¼°å®Œæˆ\n",
      "\n",
      "ğŸ“‹ è¯„ä¼°åº”ç”¨ ID: 1498229813\n",
      "âœ… æ‰¾åˆ° 6 ä¸ªé—®é¢˜çš„è¾“å‡º\n",
      "ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:14<00:00,  2.35s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:09<00:00,  1.60s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Haystackè¯„ä¼°å®Œæˆ\n",
      "\n",
      "ğŸ“‹ è¯„ä¼°åº”ç”¨ ID: 1588978095\n",
      "âœ… æ‰¾åˆ° 6 ä¸ªé—®é¢˜çš„è¾“å‡º\n",
      "ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:11<00:00,  1.87s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:09<00:00,  1.57s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Haystackè¯„ä¼°å®Œæˆ\n",
      "\n",
      "ğŸ“‹ è¯„ä¼°åº”ç”¨ ID: 1665348316\n",
      "âœ… æ‰¾åˆ° 6 ä¸ªé—®é¢˜çš„è¾“å‡º\n",
      "ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:09<00:00,  1.55s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:10<00:00,  1.67s/it]\n",
      "PromptBuilder has 3 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Haystackè¯„ä¼°å®Œæˆ\n",
      "\n",
      "ğŸ“‹ è¯„ä¼°åº”ç”¨ ID: 6447095050\n",
      "âœ… æ‰¾åˆ° 6 ä¸ªé—®é¢˜çš„è¾“å‡º\n",
      "ğŸ¯ è¿è¡ŒHaystackè¯„ä¼°pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:06<00:06,  2.10s/it]"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ Specialized RAG Evaluation Pipeline for Privacy Policy Questions\n",
    "print(\"ğŸ¯ Creating specialized RAG evaluation pipeline for privacy policy questions...\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from haystack import Pipeline\n",
    "from haystack.components.evaluators.document_mrr import DocumentMRREvaluator\n",
    "from haystack.components.evaluators.faithfulness import FaithfulnessEvaluator\n",
    "from haystack.components.evaluators.sas_evaluator import SASEvaluator\n",
    "\n",
    "# Define questions and their corresponding ground truth format\n",
    "PRIVACY_POLICY_QUESTIONS = {\n",
    "    'q1': {\n",
    "        'text': \"1. Does the app declare the collection of data?\",\n",
    "        'type': 'binary',\n",
    "        'expected_simple_answers': {'y': 'Yes', 'n': 'No'}\n",
    "    },\n",
    "    'q2': {\n",
    "        'text': \"2. What type of data does it collect?\",\n",
    "        'type': 'open'  # Open-ended question, groundtruth is free text\n",
    "    },\n",
    "    'q3': {\n",
    "        'text': \"3. Does the app declare the purpose of data collection and use?\",\n",
    "        'type': 'binary',\n",
    "        'expected_simple_answers': {'y': 'Yes', 'n': 'No'}\n",
    "    },\n",
    "    'q4': {\n",
    "        'text': \"4. Can you opt out of data collection or delete data?\",\n",
    "        'type': 'binary',\n",
    "        'expected_simple_answers': {'y': 'Yes', 'n': 'No'}\n",
    "    },\n",
    "    'q5': {\n",
    "        'text': \"5. Does the app share data with third parties?\",\n",
    "        'type': 'binary',\n",
    "        'expected_simple_answers': {'y': 'Yes', 'n': 'No'}\n",
    "    },\n",
    "    'q6': {\n",
    "        'text': \"6. If the app shares data with third parties, what third parties does the app share data with?\",\n",
    "        'type': 'open'  # Open-ended question, groundtruth is free text\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_groundtruth_data(groundtruth_path='../groundtruth.json'):\n",
    "    \"\"\"Load groundtruth data\"\"\"\n",
    "    with open(groundtruth_path, 'r', encoding='utf-8') as f:\n",
    "        gt_data = json.load(f)\n",
    "    \n",
    "    # Convert to more usable format\n",
    "    gt_dict = {}\n",
    "    for item in gt_data:\n",
    "        app_id = item['id']\n",
    "        gt_dict[app_id] = {\n",
    "            'q1': item['q1'],\n",
    "            'q2': item['q2'], \n",
    "            'q3': item['q3'],\n",
    "            'q4': item['q4'],\n",
    "            'q5': item['q5'],\n",
    "            'q6': item['q6']\n",
    "        }\n",
    "    \n",
    "    print(f\"âœ… Loaded groundtruth annotations for {len(gt_dict)} applications\")\n",
    "    return gt_dict\n",
    "\n",
    "def load_rag_output(app_id, outputs_dir='outputs'):\n",
    "    \"\"\"Load RAG output for a single application\"\"\"\n",
    "    output_file = os.path.join(outputs_dir, f\"{app_id}.json\")\n",
    "    \n",
    "    if not os.path.exists(output_file):\n",
    "        print(f\"âš ï¸ Output file does not exist: {output_file}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            rag_data = json.load(f)\n",
    "        return rag_data\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load RAG output {output_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_question_mapping(rag_outputs):\n",
    "    \"\"\"Extract question mapping from RAG output (mapping by question number prefix 1.~6.)\"\"\"\n",
    "    question_mapping = {}\n",
    "    for i, output in enumerate(rag_outputs):\n",
    "        try:\n",
    "            qtext = output['reply']['question'].strip()\n",
    "            if qtext.startswith(\"1.\"):\n",
    "                question_mapping['q1'] = output\n",
    "            elif qtext.startswith(\"2.\"):\n",
    "                question_mapping['q2'] = output\n",
    "            elif qtext.startswith(\"3.\"):\n",
    "                question_mapping['q3'] = output\n",
    "            elif qtext.startswith(\"4.\"):\n",
    "                question_mapping['q4'] = output\n",
    "            elif qtext.startswith(\"5.\"):\n",
    "                question_mapping['q5'] = output\n",
    "            elif qtext.startswith(\"6.\"):\n",
    "                question_mapping['q6'] = output\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing output #{i+1}: {e}\")\n",
    "            continue\n",
    "    return question_mapping\n",
    "\n",
    "def create_privacy_evaluation_pipeline():\n",
    "    \"\"\"Create evaluation pipeline containing only Faithfulness / SAS / Context Relevance\"\"\"\n",
    "    eval_pipeline = Pipeline()\n",
    "    eval_pipeline.add_component(\"faithfulness\", FaithfulnessEvaluator())\n",
    "    eval_pipeline.add_component(\"sas_evaluator\", SASEvaluator(model=\"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "    eval_pipeline.add_component(\"context_relevance\", ContextRelevanceEvaluator())\n",
    "    return eval_pipeline\n",
    "\n",
    "def evaluate_single_app(app_id, gt_dict, outputs_dir='outputs'):\n",
    "    \"\"\"Evaluate a single application\"\"\"\n",
    "    print(f\"\\nğŸ“‹ Evaluating Application ID: {app_id}\")\n",
    "    \n",
    "    # Load data\n",
    "    gt_answers = gt_dict.get(app_id)\n",
    "    if not gt_answers:\n",
    "        print(f\"âŒ Groundtruth for application {app_id} not found\")\n",
    "        return None\n",
    "    \n",
    "    rag_outputs = load_rag_output(app_id, outputs_dir)\n",
    "    if not rag_outputs:\n",
    "        print(f\"âŒ RAG output for application {app_id} not found\")  \n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
